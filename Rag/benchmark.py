#!/usr/bin/env python3
"""
RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•
æµ‹è¯•ä¸åŒé…ç½®ä¸‹çš„æ€§èƒ½è¡¨ç°
"""

import os
import sys
import time
import json
import statistics
from pathlib import Path
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor
import matplotlib.pyplot as plt
import numpy as np

# æ·»åŠ srcç›®å½•åˆ°Pythonè·¯å¾„
sys.path.append(str(Path(__file__).parent / "src"))

# é¢œè‰²è¾“å‡º
class Colors:
    GREEN = '\033[92m'
    YELLOW = '\033[93m'
    RED = '\033[91m'
    BLUE = '\033[94m'
    RESET = '\033[0m'
    BOLD = '\033[1m'


def print_color(text, color):
    """æ‰“å°å½©è‰²æ–‡æœ¬"""
    print(f"{color}{text}{Colors.RESET}")


class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœç±»"""

    def __init__(self, name):
        self.name = name
        self.response_times = []
        self.indexing_time = None
        self.memory_usage = []
        self.error_count = 0
        self.total_queries = 0

    def add_response_time(self, time):
        """æ·»åŠ å“åº”æ—¶é—´"""
        self.response_times.append(time)

    def get_stats(self):
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        if not self.response_times:
            return None

        return {
            "avg": statistics.mean(self.response_times),
            "median": statistics.median(self.response_times),
            "min": min(self.response_times),
            "max": max(self.response_times),
            "std": statistics.stdev(self.response_times) if len(self.response_times) > 1 else 0,
            "p95": np.percentile(self.response_times, 95),
            "p99": np.percentile(self.response_times, 99)
        }


class RAGBenchmark:
    """RAGç³»ç»ŸåŸºå‡†æµ‹è¯•å™¨"""

    def __init__(self):
        self.results = {}
        self.test_queries = [
            "ä»€ä¹ˆæ˜¯RAGç³»ç»Ÿï¼Ÿ",
            "æœºå™¨å­¦ä¹ æœ‰å“ªäº›ç±»å‹ï¼Ÿ",
            "æ·±åº¦å­¦ä¹ çš„åº”ç”¨æœ‰å“ªäº›ï¼Ÿ",
            "è‡ªç„¶è¯­è¨€å¤„ç†æ˜¯ä»€ä¹ˆï¼Ÿ",
            "è®¡ç®—æœºè§†è§‰çš„åº”ç”¨åœºæ™¯ï¼Ÿ",
            "RAGç³»ç»Ÿçš„ç»„æˆéƒ¨åˆ†ï¼Ÿ",
            "å¦‚ä½•è¯„ä¼°RAGç³»ç»Ÿï¼Ÿ",
            "å‘é‡æ•°æ®åº“çš„é€‰æ‹©ï¼Ÿ",
            "åµŒå…¥æ¨¡å‹çš„æ¯”è¾ƒï¼Ÿ",
            "æ£€ç´¢ç­–ç•¥çš„ä¼˜åŒ–ï¼Ÿ",
            "AIçš„å‘å±•å†å²ï¼Ÿ",
            "ç›‘ç£å­¦ä¹ çš„ç‰¹ç‚¹ï¼Ÿ",
            "æ— ç›‘ç£å­¦ä¹ çš„åº”ç”¨ï¼Ÿ",
            "å¼ºåŒ–å­¦ä¹ çš„åŸç†ï¼Ÿ",
            "ç¥ç»ç½‘ç»œçš„ç»“æ„ï¼Ÿ"
        ]

    def benchmark_vector_stores(self):
        """æµ‹è¯•ä¸åŒå‘é‡æ•°æ®åº“çš„æ€§èƒ½"""
        print_color("\n=== å‘é‡æ•°æ®åº“æ€§èƒ½å¯¹æ¯” ===", Colors.BOLD + Colors.BLUE)

        vector_stores = ["chroma", "faiss"]

        for store_type in vector_stores:
            print_color(f"\næµ‹è¯• {store_type.upper()}...", Colors.YELLOW)
            result = BenchmarkResult(f"vector_store_{store_type}")

            try:
                from rag_system import RAGSystem, RAGConfig

                # åˆ›å»ºé…ç½®
                config = RAGConfig(
                    data_path="./data/sample_documents",
                    vector_store_type=store_type,
                    embedding_model="openai",
                    chunk_size=1000,
                    top_k=5
                )

                # åˆ›å»ºRAGç³»ç»Ÿ
                rag = RAGSystem(config)

                # æµ‹è¯•ç´¢å¼•æ—¶é—´
                print("  ç´¢å¼•æ–‡æ¡£...")
                start_time = time.time()
                rag.index_documents()
                result.indexing_time = time.time() - start_time
                print(f"  ç´¢å¼•æ—¶é—´: {result.indexing_time:.2f}ç§’")

                # æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•
                print("  æ‰§è¡ŒæŸ¥è¯¢æµ‹è¯•...")
                for query in self.test_queries[:5]:  # ä½¿ç”¨å‰5ä¸ªæŸ¥è¯¢
                    start_time = time.time()
                    rag_result = rag.query(query)
                    response_time = time.time() - start_time

                    if rag_result["success"]:
                        result.add_response_time(response_time)
                        result.total_queries += 1
                    else:
                        result.error_count += 1

                self.results[result.name] = result
                print(f"  å¹³å‡å“åº”æ—¶é—´: {result.get_stats()['avg']:.2f}ç§’")

            except Exception as e:
                print_color(f"  é”™è¯¯: {str(e)}", Colors.RED)

    def benchmark_embeddings(self):
        """æµ‹è¯•ä¸åŒåµŒå…¥æ¨¡å‹çš„æ€§èƒ½"""
        print_color("\n=== åµŒå…¥æ¨¡å‹æ€§èƒ½å¯¹æ¯” ===", Colors.BOLD + Colors.BLUE)

        embeddings = [
            ("openai", "text-embedding-ada-002"),
            ("huggingface", "sentence-transformers/all-MiniLM-L6-v2")
        ]

        for embed_type, embed_name in embeddings:
            print_color(f"\næµ‹è¯• {embed_name}...", Colors.YELLOW)
            result = BenchmarkResult(f"embedding_{embed_type}")

            try:
                from rag_system import RAGSystem, RAGConfig

                config = RAGConfig(
                    data_path="./data/sample_documents",
                    vector_store_type="chroma",
                    embedding_model=embed_type,
                    embedding_model_name=embed_name,
                    chunk_size=1000,
                    top_k=5
                )

                rag = RAGSystem(config)

                # ç´¢å¼•
                start_time = time.time()
                rag.index_documents()
                result.indexing_time = time.time() - start_time

                # æŸ¥è¯¢æµ‹è¯•
                for query in self.test_queries[:5]:
                    start_time = time.time()
                    rag_result = rag.query(query)
                    response_time = time.time() - start_time

                    if rag_result["success"]:
                        result.add_response_time(response_time)
                        result.total_queries += 1
                    else:
                        result.error_count += 1

                self.results[result.name] = result
                print(f"  å¹³å‡å“åº”æ—¶é—´: {result.get_stats()['avg']:.2f}ç§’")

            except Exception as e:
                print_color(f"  é”™è¯¯: {str(e)}", Colors.RED)

    def benchmark_retrieval_strategies(self):
        """æµ‹è¯•ä¸åŒæ£€ç´¢ç­–ç•¥çš„æ€§èƒ½"""
        print_color("\n=== æ£€ç´¢ç­–ç•¥æ€§èƒ½å¯¹æ¯” ===", Colors.BOLD + Colors.BLUE)

        strategies = ["semantic", "multi_query", "contextual"]

        for strategy in strategies:
            print_color(f"\næµ‹è¯• {strategy} ç­–ç•¥...", Colors.YELLOW)
            result = BenchmarkResult(f"strategy_{strategy}")

            try:
                from rag_system import RAGSystem, RAGConfig

                config = RAGConfig(
                    data_path="./data/sample_documents",
                    vector_store_type="chroma",
                    embedding_model="openai",
                    retrieval_strategy=strategy,
                    chunk_size=1000,
                    top_k=5
                )

                rag = RAGSystem(config)

                # ç´¢å¼•
                start_time = time.time()
                rag.index_documents()
                result.indexing_time = time.time() - start_time

                # æŸ¥è¯¢æµ‹è¯•
                for query in self.test_queries[:5]:
                    start_time = time.time()
                    rag_result = rag.query(query)
                    response_time = time.time() - start_time

                    if rag_result["success"]:
                        result.add_response_time(response_time)
                        result.total_queries += 1
                    else:
                        result.error_count += 1

                self.results[result.name] = result
                print(f"  å¹³å‡å“åº”æ—¶é—´: {result.get_stats()['avg']:.2f}ç§’")

            except Exception as e:
                print_color(f"  é”™è¯¯: {str(e)}", Colors.RED)

    def benchmark_chunk_sizes(self):
        """æµ‹è¯•ä¸åŒæ–‡æ¡£å—å¤§å°çš„æ€§èƒ½"""
        print_color("\n=== æ–‡æ¡£å—å¤§å°æ€§èƒ½å¯¹æ¯” ===", Colors.BOLD + Colors.BLUE)

        chunk_sizes = [500, 1000, 1500, 2000]

        for size in chunk_sizes:
            print_color(f"\næµ‹è¯•å—å¤§å° {size}...", Colors.YELLOW)
            result = BenchmarkResult(f"chunk_size_{size}")

            try:
                from rag_system import RAGSystem, RAGConfig

                config = RAGConfig(
                    data_path="./data/sample_documents",
                    vector_store_type="chroma",
                    embedding_model="openai",
                    chunk_size=size,
                    chunk_overlap=50,
                    top_k=5
                )

                rag = RAGSystem(config)

                # ç´¢å¼•
                start_time = time.time()
                rag.index_documents()
                result.indexing_time = time.time() - start_time

                # æŸ¥è¯¢æµ‹è¯•
                for query in self.test_queries[:5]:
                    start_time = time.time()
                    rag_result = rag.query(query)
                    response_time = time.time() - start_time

                    if rag_result["success"]:
                        result.add_response_time(response_time)
                        result.total_queries += 1
                    else:
                        result.error_count += 1

                self.results[result.name] = result
                print(f"  å¹³å‡å“åº”æ—¶é—´: {result.get_stats()['avg']:.2f}ç§’")

            except Exception as e:
                print_color(f"  é”™è¯¯: {str(e)}", Colors.RED)

    def benchmark_concurrent_queries(self):
        """æµ‹è¯•å¹¶å‘æŸ¥è¯¢æ€§èƒ½"""
        print_color("\n=== å¹¶å‘æŸ¥è¯¢æ€§èƒ½æµ‹è¯• ===", Colors.BOLD + Colors.BLUE)

        try:
            from rag_system import create_rag_system

            # åˆ›å»ºRAGç³»ç»Ÿ
            rag = create_rag_system(
                data_path="./data/sample_documents",
                vector_store_type="chroma",
                embedding_model="openai"
            )

            # æµ‹è¯•ä¸åŒçš„å¹¶å‘çº§åˆ«
            concurrency_levels = [1, 2, 5, 10]

            for level in concurrency_levels:
                print_color(f"\næµ‹è¯•å¹¶å‘çº§åˆ« {level}...", Colors.YELLOW)
                result = BenchmarkResult(f"concurrent_{level}")

                def execute_query(query):
                    """æ‰§è¡Œå•ä¸ªæŸ¥è¯¢"""
                    start_time = time.time()
                    rag_result = rag.query(query)
                    response_time = time.time() - start_time
                    return response_time, rag_result["success"]

                # å¹¶å‘æ‰§è¡ŒæŸ¥è¯¢
                start_time = time.time()
                with ThreadPoolExecutor(max_workers=level) as executor:
                    futures = [
                        executor.submit(execute_query, query)
                        for query in self.test_queries[:10]  # ä½¿ç”¨10ä¸ªæŸ¥è¯¢
                    ]

                    for future in futures:
                        response_time, success = future.result()
                        if success:
                            result.add_response_time(response_time)
                            result.total_queries += 1
                        else:
                            result.error_count += 1

                total_time = time.time() - start_time
                result.total_execution_time = total_time

                print(f"  æ€»æ‰§è¡Œæ—¶é—´: {total_time:.2f}ç§’")
                print(f"  å¹³å‡å“åº”æ—¶é—´: {result.get_stats()['avg']:.2f}ç§’")
                print(f"  QPS: {result.total_queries / total_time:.2f}")

                self.results[result.name] = result

        except Exception as e:
            print_color(f"å¹¶å‘æµ‹è¯•é”™è¯¯: {str(e)}", Colors.RED)

    def generate_report(self):
        """ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š"""
        print_color("\n=== ç”Ÿæˆæµ‹è¯•æŠ¥å‘Š ===", Colors.BOLD + Colors.BLUE)

        # æ–‡æœ¬æŠ¥å‘Š
        report_lines = []
        report_lines.append("RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
        report_lines.append("="*50)
        report_lines.append(f"æµ‹è¯•æ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        report_lines.append("")

        # ç»Ÿè®¡å„é¡¹æµ‹è¯•ç»“æœ
        for name, result in self.results.items():
            stats = result.get_stats()
            if stats:
                report_lines.append(f"{name}:")
                report_lines.append(f"  å¹³å‡å“åº”æ—¶é—´: {stats['avg']:.2f}ç§’")
                report_lines.append(f"  ä¸­ä½æ•°: {stats['median']:.2f}ç§’")
                report_lines.append(f"  æœ€å°å€¼: {stats['min']:.2f}ç§’")
                report_lines.append(f"  æœ€å¤§å€¼: {stats['max']:.2f}ç§’")
                report_lines.append(f"  æ ‡å‡†å·®: {stats['std']:.2f}ç§’")
                report_lines.append(f"  95%åˆ†ä½æ•°: {stats['p95']:.2f}ç§’")
                if result.indexing_time:
                    report_lines.append(f"  ç´¢å¼•æ—¶é—´: {result.indexing_time:.2f}ç§’")
                report_lines.append(f"  æˆåŠŸç‡: {(result.total_queries / (result.total_queries + result.error_count) * 100):.1f}%")
                report_lines.append("")

        # ä¿å­˜æŠ¥å‘Š
        report_file = f"benchmark_report_{datetime.now().strftime('%Y%m%d_%H%M%S')}.txt"
        with open(report_file, 'w', encoding='utf-8') as f:
            f.write('\n'.join(report_lines))

        # JSONæ ¼å¼æŠ¥å‘Š
        json_results = {}
        for name, result in self.results.items():
            json_results[name] = {
                "stats": result.get_stats(),
                "indexing_time": result.indexing_time,
                "total_queries": result.total_queries,
                "error_count": result.error_count
            }

        json_file = f"benchmark_results_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        with open(json_file, 'w', encoding='utf-8') as f:
            json.dump(json_results, f, ensure_ascii=False, indent=2)

        print_color(f"æŠ¥å‘Šå·²ä¿å­˜:", Colors.GREEN)
        print(f"  æ–‡æœ¬æŠ¥å‘Š: {report_file}")
        print(f"  JSONæŠ¥å‘Š: {json_file}")

        # ç”Ÿæˆå¯è§†åŒ–å›¾è¡¨
        self.generate_plots()

    def generate_plots(self):
        """ç”Ÿæˆæ€§èƒ½å›¾è¡¨"""
        try:
            # æŒ‰æµ‹è¯•ç±»å‹åˆ†ç»„ç»“æœ
            vector_store_results = {}
            embedding_results = {}
            strategy_results = {}
            chunk_results = {}
            concurrent_results = {}

            for name, result in self.results.items():
                stats = result.get_stats()
                if not stats:
                    continue

                if name.startswith("vector_store_"):
                    vector_store_results[name.replace("vector_store_", "")] = stats['avg']
                elif name.startswith("embedding_"):
                    embedding_results[name.replace("embedding_", "")] = stats['avg']
                elif name.startswith("strategy_"):
                    strategy_results[name.replace("strategy_", "")] = stats['avg']
                elif name.startswith("chunk_size_"):
                    size = name.replace("chunk_size_", "")
                    chunk_results[int(size)] = stats['avg']
                elif name.startswith("concurrent_"):
                    concurrent_results[name.replace("concurrent_", "")] = stats['avg']

            # åˆ›å»ºå›¾è¡¨
            if vector_store_results or embedding_results or strategy_results:
                fig, axes = plt.subplots(2, 2, figsize=(12, 10))
                fig.suptitle('RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•', fontsize=16)

                # å‘é‡æ•°æ®åº“å¯¹æ¯”
                if vector_store_results:
                    axes[0, 0].bar(vector_store_results.keys(), vector_store_results.values())
                    axes[0, 0].set_title('å‘é‡æ•°æ®åº“å“åº”æ—¶é—´å¯¹æ¯”')
                    axes[0, 0].set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')

                # åµŒå…¥æ¨¡å‹å¯¹æ¯”
                if embedding_results:
                    axes[0, 1].bar(embedding_results.keys(), embedding_results.values())
                    axes[0, 1].set_title('åµŒå…¥æ¨¡å‹å“åº”æ—¶é—´å¯¹æ¯”')
                    axes[0, 1].set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')

                # æ£€ç´¢ç­–ç•¥å¯¹æ¯”
                if strategy_results:
                    axes[1, 0].bar(strategy_results.keys(), strategy_results.values())
                    axes[1, 0].set_title('æ£€ç´¢ç­–ç•¥å“åº”æ—¶é—´å¯¹æ¯”')
                    axes[1, 0].set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')

                # æ–‡æ¡£å—å¤§å°å¯¹æ¯”
                if chunk_results:
                    sizes = sorted(chunk_results.keys())
                    times = [chunk_results[size] for size in sizes]
                    axes[1, 1].plot(sizes, times, marker='o')
                    axes[1, 1].set_title('æ–‡æ¡£å—å¤§å°å¯¹å“åº”æ—¶é—´çš„å½±å“')
                    axes[1, 1].set_xlabel('å—å¤§å°')
                    axes[1, 1].set_ylabel('å¹³å‡å“åº”æ—¶é—´ (ç§’)')

                plt.tight_layout()
                plot_file = f"benchmark_plots_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
                plt.savefig(plot_file)
                print_color(f"æ€§èƒ½å›¾è¡¨å·²ä¿å­˜: {plot_file}", Colors.GREEN)

        except Exception as e:
            print_color(f"ç”Ÿæˆå›¾è¡¨å¤±è´¥: {str(e)}", Colors.RED)

    def run_all_benchmarks(self):
        """è¿è¡Œæ‰€æœ‰åŸºå‡†æµ‹è¯•"""
        print_color("\nğŸš€ å¼€å§‹RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•", Colors.BOLD + Colors.BLUE)
        print("="*60)

        # æ£€æŸ¥APIå¯†é’¥
        if not os.getenv("OPENAI_API_KEY"):
            print_color("âŒ è¯·è®¾ç½®OPENAI_API_KEYç¯å¢ƒå˜é‡", Colors.RED)
            return

        start_time = time.time()

        # è¿è¡Œå„é¡¹æµ‹è¯•
        self.benchmark_vector_stores()
        self.benchmark_embeddings()
        self.benchmark_retrieval_strategies()
        self.benchmark_chunk_sizes()
        self.benchmark_concurrent_queries()

        total_time = time.time() - start_time
        print_color(f"\nâœ… æ‰€æœ‰æµ‹è¯•å®Œæˆï¼æ€»è€—æ—¶: {total_time:.2f}ç§’", Colors.GREEN)

        # ç”ŸæˆæŠ¥å‘Š
        self.generate_report()


def main():
    """ä¸»å‡½æ•°"""
    print_color("RAGç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•å·¥å…·", Colors.BOLD + Colors.CYAN)
    print("="*60)

    benchmark = RAGBenchmark()

    # é€‰æ‹©æµ‹è¯•ç±»å‹
    print("\nè¯·é€‰æ‹©è¦æ‰§è¡Œçš„æµ‹è¯•:")
    options = [
        "è¿è¡Œæ‰€æœ‰æµ‹è¯•",
        "å‘é‡æ•°æ®åº“å¯¹æ¯”",
        "åµŒå…¥æ¨¡å‹å¯¹æ¯”",
        "æ£€ç´¢ç­–ç•¥å¯¹æ¯”",
        "æ–‡æ¡£å—å¤§å°å¯¹æ¯”",
        "å¹¶å‘æ€§èƒ½æµ‹è¯•"
    ]

    choice = input("\nè¯·é€‰æ‹© (1-6): ").strip()

    if choice == "1":
        benchmark.run_all_benchmarks()
    elif choice == "2":
        benchmark.benchmark_vector_stores()
        benchmark.generate_report()
    elif choice == "3":
        benchmark.benchmark_embeddings()
        benchmark.generate_report()
    elif choice == "4":
        benchmark.benchmark_retrieval_strategies()
        benchmark.generate_report()
    elif choice == "5":
        benchmark.benchmark_chunk_sizes()
        benchmark.generate_report()
    elif choice == "6":
        benchmark.benchmark_concurrent_queries()
        benchmark.generate_report()
    else:
        print_color("æ— æ•ˆé€‰æ‹©", Colors.RED)
        return

    print_color("\næµ‹è¯•å®Œæˆï¼è¯·æŸ¥çœ‹ç”Ÿæˆçš„æŠ¥å‘Šæ–‡ä»¶ã€‚", Colors.GREEN)


if __name__ == "__main__":
    main()