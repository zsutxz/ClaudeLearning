# RAGç³»ç»Ÿ - å®Œæ•´éƒ¨ç½²æŒ‡å—

æœ¬é¡¹ç›®æä¾›äº†å¤šç§RAGï¼ˆRetrieval-Augmented Generationï¼‰ç³»ç»Ÿçš„å®ç°æ–¹æ¡ˆï¼Œä»ç®€å•çš„æœ¬åœ°åµŒå…¥åˆ°ä¸“ä¸šçš„Sentence-Transformerséƒ¨ç½²ã€‚

## ğŸ“‹ ç›®å½•

- [é¡¹ç›®æ¦‚è¿°](#é¡¹ç›®æ¦‚è¿°)
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [å¿«é€Ÿå¼€å§‹](#å¿«é€Ÿå¼€å§‹)
- [éƒ¨ç½²æ–¹æ¡ˆ](#éƒ¨ç½²æ–¹æ¡ˆ)
- [ç¤ºä¾‹ä»£ç ](#ç¤ºä¾‹ä»£ç )
- [æ€§èƒ½å¯¹æ¯”](#æ€§èƒ½å¯¹æ¯”)
- [å¸¸è§é—®é¢˜](#å¸¸è§é—®é¢˜)
- [è¿›é˜¶ç”¨æ³•](#è¿›é˜¶ç”¨æ³•)

## ğŸ¯ é¡¹ç›®æ¦‚è¿°

æœ¬é¡¹ç›®åŒ…å«å¤šç§RAGå®ç°æ–¹æ¡ˆï¼š

1. **DeepSeek APIæ–¹æ¡ˆ** (`test_deepseek_rag.py`) - ä½¿ç”¨DeepSeekçš„èŠå¤©API
2. **TF-IDFæœ¬åœ°æ–¹æ¡ˆ** (`test_local_embedding_simple.py`) - è½»é‡çº§æœ¬åœ°å®ç°
3. **Sentence-Transformersæ–¹æ¡ˆ** (`test_sentence_transformers_rag.py`) - ä¸“ä¸šçš„æœ¬åœ°åµŒå…¥
4. **å®Œæ•´RAGç³»ç»Ÿ** (`src/rag_system_final.py`) - æ”¯æŒå¤šç§å‘é‡æ•°æ®åº“çš„å®Œæ•´å®ç°

## ğŸš€ ç‰¹æ€§

- **å¤šç§å‘é‡æ•°æ®åº“æ”¯æŒ**ï¼šChroma, FAISS, Pinecone, Weaviate, Qdrant
- **å¤šç§åµŒå…¥æ¨¡å‹**ï¼šOpenAI, HuggingFace, Sentence-Transformers, TF-IDF
- **çµæ´»çš„æ£€ç´¢ç­–ç•¥**ï¼šè¯­ä¹‰æœç´¢ã€æ··åˆæœç´¢ã€å¤šæŸ¥è¯¢æ£€ç´¢ã€ä¸Šä¸‹æ–‡å‹ç¼©
- **å®Œæ•´çš„æ–‡æ¡£å¤„ç†**ï¼šæ”¯æŒPDFã€Markdownã€Wordã€CSVç­‰æ ¼å¼
- **æœ¬åœ°éƒ¨ç½²é€‰é¡¹**ï¼šå®Œå…¨ç¦»çº¿è¿è¡Œï¼Œä¿æŠ¤æ•°æ®éšç§
- **æ˜“äºä½¿ç”¨**ï¼šç®€æ´çš„APIå’Œè¯¦ç»†çš„æ–‡æ¡£

## ğŸ“ é¡¹ç›®ç»“æ„

```
rag/
â”œâ”€â”€ src/                       # æºä»£ç 
â”‚   â”œâ”€â”€ rag_system_final.py    # å®Œæ•´RAGç³»ç»Ÿå®ç°
â”‚   â”œâ”€â”€ simple_rag.py          # ç®€åŒ–ç‰ˆRAGç³»ç»Ÿ
â”‚   â””â”€â”€ evaluation.py          # è¯„ä¼°æ¨¡å—
â”œâ”€â”€ data/                      # æ•°æ®ç›®å½•
â”‚   â””â”€â”€ sample_documents/      # ç¤ºä¾‹æ–‡æ¡£
â”œâ”€â”€ test_deepseek_rag.py       # DeepSeek APIç‰ˆæœ¬
â”œâ”€â”€ test_local_embedding_simple.py  # TF-IDFæœ¬åœ°ç‰ˆæœ¬
â”œâ”€â”€ test_sentence_transformers_rag.py # Sentence-Transformersç‰ˆæœ¬
â”œâ”€â”€ test_openai_rag.py         # OpenAIç‰ˆæœ¬æµ‹è¯•
â”œâ”€â”€ deploy_sentence_transformers_guide.md  # è¯¦ç»†éƒ¨ç½²æŒ‡å—
â””â”€â”€ README.md                  # é¡¹ç›®è¯´æ˜
```

## ğŸ”§ ç¯å¢ƒå‡†å¤‡

### åŸºç¡€ä¾èµ–

```bash
# å¿…éœ€çš„PythonåŒ…
pip install langchain langchain-community chromadb
pip install openai python-dotenv numpy scikit-learn
```

### Sentence-Transformers ä¾èµ–

```bash
# å®‰è£…sentence-transformerså’ŒPyTorch
pip install sentence-transformers torch

# GPUæ”¯æŒï¼ˆå¯é€‰ï¼Œæå‡æ€§èƒ½ï¼‰
pip install sentence-transformers torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118
```

### ç¯å¢ƒå˜é‡é…ç½®

åˆ›å»º `.env` æ–‡ä»¶ï¼š

```env
# DeepSeek APIé…ç½®
DEEPSEEK_API_KEY=your_deepseek_api_key_here
DEEPSEEK_BASE_URL=https://api.deepseek.com

# OpenAI APIï¼ˆå¦‚æœéœ€è¦ï¼‰
OPENAI_API_KEY=your_openai_api_key_here

# Hugging Faceé•œåƒï¼ˆå›½å†…åŠ é€Ÿï¼‰
HF_ENDPOINT=https://hf-mirror.com
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

### 1. æµ‹è¯•æœ¬åœ°TF-IDFæ–¹æ¡ˆï¼ˆæ— éœ€ç½‘ç»œï¼‰

```bash
python test_local_embedding_simple.py
```

**ç‰¹ç‚¹**ï¼š
- âœ… å®Œå…¨æœ¬åœ°è¿è¡Œ
- âœ… æ— éœ€ä¸‹è½½æ¨¡å‹
- âœ… é€‚åˆå¿«é€ŸéªŒè¯

### 2. æµ‹è¯•DeepSeek APIæ–¹æ¡ˆ

```bash
python test_deepseek_rag.py
```

**æ³¨æ„**ï¼šéœ€è¦é…ç½®DeepSeek APIå¯†é’¥

### 3. æµ‹è¯•Sentence-Transformersæ–¹æ¡ˆï¼ˆæ¨èï¼‰

```bash
python test_sentence_transformers_rag.py
```

**é¦–æ¬¡è¿è¡Œ**ä¼šè‡ªåŠ¨ä¸‹è½½æ¨¡å‹ï¼Œè¯·è€å¿ƒç­‰å¾…ã€‚

### 4. ä½¿ç”¨å®Œæ•´RAGç³»ç»Ÿ

```bash
# ç¦»çº¿ç‰ˆæœ¬
python test_offline_rag.py

# OpenAIç‰ˆæœ¬ï¼ˆéœ€è¦APIå¯†é’¥ï¼‰
export OPENAI_API_KEY=your_key_here
python test_openai_rag.py
```

## ğŸ“¦ éƒ¨ç½²æ–¹æ¡ˆ

### æ–¹æ¡ˆä¸€ï¼šSentence-Transformersï¼ˆæ¨èï¼‰

è¿™æ˜¯æœ€é€‚åˆç”Ÿäº§ç¯å¢ƒçš„æ–¹æ¡ˆï¼Œæä¾›é«˜è´¨é‡çš„è¯­ä¹‰ç†è§£ã€‚

#### 1.1 æ¨¡å‹é€‰æ‹©

| æ¨¡å‹åç§° | å¤§å° | ç‰¹ç‚¹ | é€‚ç”¨åœºæ™¯ |
|---------|------|------|---------|
| `shibing624/text2vec-base-chinese` | 420MB | ä¸­æ–‡ä¼˜åŒ– | ä¸­æ–‡ä¸ºä¸»çš„åº”ç”¨ |
| `paraphrase-multilingual-MiniLM-L12-v2` | 420MB | å¤šè¯­è¨€è½»é‡ | å›½é™…åŒ–åº”ç”¨ |
| `all-mpnet-base-v2` | 420MB | è‹±æ–‡é«˜è´¨é‡ | è‹±æ–‡åº”ç”¨ |
| `paraphrase-multilingual-mpnet-base-v2` | 1.1GB | å¤šè¯­è¨€é«˜è´¨é‡ | å¯¹è´¨é‡è¦æ±‚é«˜çš„åœºæ™¯ |

#### 1.2 å®ç°ä»£ç 

```python
from sentence_transformers import SentenceTransformer
import numpy as np
from sklearn.metrics.pairwise import cosine_similarity

class SentenceTransformersEmbeddings:
    """ä¸“ä¸šçš„æœ¬åœ°åµŒå…¥å®ç°"""

    def __init__(self, model_name="paraphrase-multilingual-MiniLM-L12-v2"):
        print(f"åŠ è½½æ¨¡å‹: {model_name}")
        self.model = SentenceTransformer(model_name)

    def embed_query(self, text):
        """ç”ŸæˆæŸ¥è¯¢åµŒå…¥"""
        return self.model.encode(text)

    def embed_documents(self, texts, batch_size=32):
        """æ‰¹é‡ç”Ÿæˆæ–‡æ¡£åµŒå…¥"""
        return self.model.encode(texts, batch_size=batch_size)

    def similarity_search(self, query, documents, k=3):
        """è¯­ä¹‰æœç´¢"""
        query_emb = self.embed_query(query)
        doc_embs = self.embed_documents([doc.page_content for doc in documents])

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity([query_emb], doc_embs)[0]

        # è·å–Top-Kç»“æœ
        top_indices = np.argsort(similarities)[::-1][:k]

        results = []
        for idx in top_indices:
            results.append({
                'document': documents[idx],
                'similarity': similarities[idx]
            })

        return results

# ä½¿ç”¨ç¤ºä¾‹
embeddings = SentenceTransformersEmbeddings("shibing624/text2vec-base-chinese")
results = embeddings.similarity_search("ä»€ä¹ˆæ˜¯RAGï¼Ÿ", documents)
```

#### 1.3 é›†æˆåˆ°ChromaDB

```python
from langchain_community.vectorstores import Chroma

# åˆ›å»ºå‘é‡å­˜å‚¨
vector_store = Chroma.from_documents(
    documents=documents,
    embedding=embeddings,  # ä½¿ç”¨ä¸Šé¢çš„embeddingså®ä¾‹
    persist_directory="./chroma_store"
)

# æœç´¢
results = vector_store.similarity_search("æŸ¥è¯¢æ–‡æœ¬", k=5)
```

### æ–¹æ¡ˆäºŒï¼šTF-IDFæœ¬åœ°æ–¹æ¡ˆ

é€‚åˆå¿«é€ŸåŸå‹å¼€å‘å’Œèµ„æºå—é™çš„ç¯å¢ƒã€‚

```python
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

class LocalTfidfEmbeddings:
    """è½»é‡çº§TF-IDFåµŒå…¥"""

    def __init__(self, max_features=1000):
        self.vectorizer = TfidfVectorizer(
            max_features=max_features,
            ngram_range=(1, 2)
        )
        self.fitted = False

    def fit(self, documents):
        """è®­ç»ƒTF-IDF"""
        texts = [doc.page_content for doc in documents]
        self.vectorizer.fit(texts)
        self.fitted = True

    def search(self, query, documents, k=3):
        """æœç´¢ç›¸å…³æ–‡æ¡£"""
        if not self.fitted:
            self.fit(documents)

        # è½¬æ¢æŸ¥è¯¢
        query_vec = self.vectorizer.transform([query])

        # è½¬æ¢æ–‡æ¡£
        doc_texts = [doc.page_content for doc in documents]
        doc_vecs = self.vectorizer.transform(doc_texts)

        # è®¡ç®—ç›¸ä¼¼åº¦
        similarities = cosine_similarity(query_vec, doc_vecs)[0]

        # è¿”å›ç»“æœ
        results = []
        for idx in np.argsort(similarities)[::-1][:k]:
            results.append({
                'document': documents[idx],
                'similarity': similarities[idx]
            })

        return results
```

### æ–¹æ¡ˆä¸‰ï¼šAPIæ··åˆæ–¹æ¡ˆ

ç»“åˆæœ¬åœ°åµŒå…¥å’Œäº‘ç«¯LLMçš„ä¼˜åŠ¿ã€‚

```python
import os
from openai import OpenAI

class HybridRAGSystem:
    """æ··åˆRAGç³»ç»Ÿï¼šæœ¬åœ°åµŒå…¥ + äº‘ç«¯LLM"""

    def __init__(self, embedding_model, api_key, base_url):
        self.embeddings = embedding_model
        self.client = OpenAI(
            api_key=api_key,
            base_url=base_url
        )

    def query(self, question, documents, k=3):
        """å®Œæ•´æŸ¥è¯¢æµç¨‹"""
        # 1. æ£€ç´¢ç›¸å…³æ–‡æ¡£
        retrieved_docs = self.embeddings.similarity_search(
            question, documents, k
        )

        # 2. æ„å»ºæç¤º
        context = "\n".join([
            doc['document'].page_content for doc in retrieved_docs
        ])

        prompt = f"""åŸºäºä»¥ä¸‹ä¿¡æ¯å›ç­”é—®é¢˜ï¼š

ä¿¡æ¯ï¼š
{context}

é—®é¢˜ï¼š{question}

å›ç­”ï¼š"""

        # 3. è°ƒç”¨LLMç”Ÿæˆç­”æ¡ˆ
        response = self.client.chat.completions.create(
            model="deepseek-chat",
            messages=[{"role": "user", "content": prompt}],
            max_tokens=500
        )

        return {
            "answer": response.choices[0].message.content,
            "sources": retrieved_docs
        }
```

## ğŸ“Š æ€§èƒ½å¯¹æ¯”

| æ–¹æ¡ˆ | åµŒå…¥è´¨é‡ | é€Ÿåº¦ | æˆæœ¬ | éšç§ | éƒ¨ç½²éš¾åº¦ |
|------|---------|------|------|------|---------|
| Sentence-Transformers | â­â­â­â­â­ | â­â­â­â­ | ä½ | â­â­â­â­â­ | â­â­â­ |
| TF-IDF | â­â­ | â­â­â­â­â­ | æœ€ä½ | â­â­â­â­â­ | â­â­â­â­â­ |
| OpenAI API | â­â­â­â­â­ | â­â­â­â­â­ | é«˜ | â­ | â­â­â­â­â­ |
| DeepSeek API | â­â­â­â­ | â­â­â­â­ | ä¸­ | â­ | â­â­â­â­ |

## ğŸ”§ é«˜çº§é…ç½®

### GPUåŠ é€Ÿ

```python
import torch

# æ£€æŸ¥GPU
device = 'cuda' if torch.cuda.is_available() else 'cpu'
print(f"ä½¿ç”¨è®¾å¤‡: {device}")

# åŠ è½½æ¨¡å‹åˆ°GPU
model = SentenceTransformer(model_name)
model = model.to(device)

# ä½¿ç”¨GPUç¼–ç 
embeddings = model.encode(texts, device=device)
```

### æ‰¹é‡å¤„ç†ä¼˜åŒ–

```python
def batch_encode(texts, batch_size=32):
    """é«˜æ•ˆçš„æ‰¹é‡ç¼–ç """
    embeddings = []
    for i in range(0, len(texts), batch_size):
        batch = texts[i:i+batch_size]
        batch_emb = model.encode(batch, show_progress_bar=True)
        embeddings.extend(batch_emb)
    return embeddings
```

### ç¼“å­˜æœºåˆ¶

```python
import pickle
import hashlib

class CachedEmbeddings:
    """å¸¦ç¼“å­˜çš„åµŒå…¥ç³»ç»Ÿ"""

    def __init__(self, model, cache_dir="./cache"):
        self.model = model
        self.cache_dir = Path(cache_dir)
        self.cache_dir.mkdir(exist_ok=True)

    def get_cache_path(self, text):
        """ç”Ÿæˆç¼“å­˜è·¯å¾„"""
        hash_key = hashlib.md5(text.encode()).hexdigest()
        return self.cache_dir / f"{hash_key}.pkl"

    def encode(self, text):
        """å¸¦ç¼“å­˜çš„ç¼–ç """
        cache_path = self.get_cache_path(text)

        # å°è¯•ä»ç¼“å­˜è¯»å–
        if cache_path.exists():
            with open(cache_path, 'rb') as f:
                return pickle.load(f)

        # ç”Ÿæˆæ–°çš„åµŒå…¥
        embedding = self.model.encode(text)

        # ä¿å­˜åˆ°ç¼“å­˜
        with open(cache_path, 'wb') as f:
            pickle.dump(embedding, f)

        return embedding
```

## ğŸŒ ç½‘ç»œé—®é¢˜è§£å†³æ–¹æ¡ˆ

### ä½¿ç”¨å›½å†…é•œåƒ

```python
import os
# ä½¿ç”¨Hugging Faceé•œåƒ
os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'
```

### ç¦»çº¿éƒ¨ç½²

1. **åœ¨æœ‰ç½‘ç»œçš„ç¯å¢ƒä¸‹è½½æ¨¡å‹**ï¼š
```python
model = SentenceTransformer('model-name')
model.save('./local_model')
```

2. **æ‰“åŒ…åˆ°ç¦»çº¿ç¯å¢ƒ**ï¼š
```bash
tar -czf sentence_transformers_model.tar.gz ./local_model
```

3. **ç¦»çº¿ç¯å¢ƒåŠ è½½**ï¼š
```python
model = SentenceTransformer('./local_model')
```

## ğŸ“š è¯¦ç»†åŠŸèƒ½

### 1. å‘é‡æ•°æ®åº“é€‰æ‹©

```python
# Chromaï¼ˆè½»é‡çº§ï¼Œæœ¬åœ°ä½¿ç”¨ï¼‰
config = RAGConfig(vector_store_type="chroma")

# FAISSï¼ˆé«˜æ€§èƒ½ï¼Œæœ¬åœ°ï¼‰
config = RAGConfig(vector_store_type="faiss")

# Pineconeï¼ˆäº‘ç«¯ï¼Œå¯æ‰©å±•ï¼‰
config = RAGConfig(
    vector_store_type="pinecone",
    pinecone_api_key="your-key",
    pinecone_environment="us-west1-gcp"
)

# Weaviateï¼ˆæ··åˆæœç´¢ï¼‰
config = RAGConfig(vector_store_type="weaviate")
```

### 2. åµŒå…¥æ¨¡å‹é€‰æ‹©

```python
# OpenAI
config = RAGConfig(embedding_model="openai", embedding_model_name="text-embedding-ada-002")

# HuggingFace Sentence Transformers
config = RAGConfig(
    embedding_model="huggingface",
    embedding_model_name="sentence-transformers/all-MiniLM-L6-v2"
)

# æœ¬åœ°Sentence-Transformers
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('shibing624/text2vec-base-chinese')
```

### 3. æ£€ç´¢ç­–ç•¥

```python
# è¯­ä¹‰æœç´¢ï¼ˆé»˜è®¤ï¼‰
config = RAGConfig(retrieval_strategy="semantic")

# å¤šæŸ¥è¯¢æ£€ç´¢ï¼ˆç”Ÿæˆå¤šä¸ªæŸ¥è¯¢å˜ä½“ï¼‰
config = RAGConfig(retrieval_strategy="multi_query")

# ä¸Šä¸‹æ–‡å‹ç¼©ï¼ˆåªä¿ç•™ç›¸å…³éƒ¨åˆ†ï¼‰
config = RAGConfig(retrieval_strategy="contextual")

# æ··åˆæœç´¢ï¼ˆè¯­ä¹‰+å…³é”®è¯ï¼‰
config = RAGConfig(retrieval_strategy="hybrid")
```

## â“ å¸¸è§é—®é¢˜

### Q1: æ¨¡å‹ä¸‹è½½å¤±è´¥æ€ä¹ˆåŠï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨å›½å†…é•œåƒï¼š`export HF_ENDPOINT=https://hf-mirror.com`
2. æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹æ–‡ä»¶
3. ä½¿ç”¨ä»£ç†

### Q2: å†…å­˜ä¸è¶³å¦‚ä½•å¤„ç†ï¼Ÿ

**è§£å†³æ–¹æ¡ˆ**ï¼š
1. ä½¿ç”¨æ›´å°çš„æ¨¡å‹ï¼ˆå¦‚MiniLMï¼‰
2. å‡å°batch_size
3. å¢åŠ è™šæ‹Ÿå†…å­˜
4. ä½¿ç”¨CPUè€ŒéGPU

### Q3: å¦‚ä½•é€‰æ‹©åˆé€‚çš„æ¨¡å‹ï¼Ÿ

**é€‰æ‹©æŒ‡å—**ï¼š
- ä¸­æ–‡ä¸ºä¸» â†’ `shibing624/text2vec-base-chinese`
- å¤šè¯­è¨€éœ€æ±‚ â†’ `paraphrase-multilingual-MiniLM-L12-v2`
- è¿½æ±‚è´¨é‡ â†’ `paraphrase-multilingual-mpnet-base-v2`
- èµ„æºå—é™ â†’ `all-MiniLM-L6-v2`

### Q4: å¦‚ä½•æé«˜æ£€ç´¢å‡†ç¡®ç‡ï¼Ÿ

**ä¼˜åŒ–æ–¹æ³•**ï¼š
1. ä½¿ç”¨æ›´é«˜è´¨é‡çš„åµŒå…¥æ¨¡å‹
2. å¢åŠ æ–‡æ¡£æ•°é‡å’Œè´¨é‡
3. ä¼˜åŒ–æŸ¥è¯¢è¯­å¥
4. ä½¿ç”¨æ··åˆæ£€ç´¢ï¼ˆBM25 + å‘é‡ï¼‰

## ğŸš€ è¿›é˜¶ç”¨æ³•

### å¾®è°ƒè‡ªå®šä¹‰æ¨¡å‹

```python
from sentence_transformers import SentenceTransformer, losses
from sentence_transformers.readers import InputExample

# å‡†å¤‡è®­ç»ƒæ•°æ®
train_examples = [
    InputExample(texts=['å¥å­1', 'ç›¸ä¼¼å¥å­'], label=1.0),
    InputExample(texts=['å¥å­1', 'ä¸ç›¸ä¼¼å¥å­'], label=0.0),
]

# åŠ è½½é¢„è®­ç»ƒæ¨¡å‹
model = SentenceTransformer('base-model')

# å®šä¹‰æŸå¤±å‡½æ•°
train_loss = losses.CosineSimilarityLoss(model)

# å¾®è°ƒ
model.fit(
    train_objectives=[(train_dataloader, train_loss)],
    epochs=3,
    warmup_steps=100,
    output_path='./fine-tuned-model'
)
```

### å¤šæ¨¡æ€RAG

```python
from sentence_transformers import SentenceTransformer, util
from PIL import Image

# åŠ è½½å¤šæ¨¡æ€æ¨¡å‹
model = SentenceTransformer('clip-ViT-B-32')

# å›¾åƒå’Œæ–‡æœ¬åµŒå…¥
image_emb = model.encode(Image.open('image.jpg'))
text_emb = model.encode('å›¾ç‰‡æè¿°')

# è®¡ç®—ç›¸ä¼¼åº¦
similarity = util.cos_sim(image_emb, text_emb)
```

## ğŸ“ æœ€ä½³å®è·µ

1. **ç”Ÿäº§ç¯å¢ƒéƒ¨ç½²**
   - ä½¿ç”¨ä¸“ä¸šçš„åµŒå…¥æ¨¡å‹
   - å®ç°ç¼“å­˜æœºåˆ¶
   - ç›‘æ§æ€§èƒ½æŒ‡æ ‡
   - å®šæœŸæ›´æ–°æ¨¡å‹

2. **æ€§èƒ½ä¼˜åŒ–**
   - GPUåŠ é€Ÿæ‰¹é‡å¤„ç†
   - åˆç†çš„batch_size
   - å‘é‡åŒ–æ“ä½œ
   - å¼‚æ­¥å¤„ç†

3. **å®‰å…¨è€ƒè™‘**
   - æœ¬åœ°éƒ¨ç½²ä¿æŠ¤æ•°æ®éšç§
   - APIå¯†é’¥å®‰å…¨ç®¡ç†
   - è¾“å…¥å†…å®¹è¿‡æ»¤
   - è®¿é—®æƒé™æ§åˆ¶

## ğŸ“š ç›¸å…³èµ„æº

- [Sentence-Transformerså®˜æ–¹æ–‡æ¡£](https://www.sbert.net/)
- [Hugging Faceæ¨¡å‹åº“](https://huggingface.co/models)
- [LangChainæ–‡æ¡£](https://python.langchain.com/)
- [ChromaDBæ–‡æ¡£](https://docs.trychroma.com/)
- [è¯¦ç»†éƒ¨ç½²æŒ‡å—](./deploy_sentence_transformers_guide.md)

## ğŸ¤ è´¡çŒ®

æ¬¢è¿æäº¤é—®é¢˜å’Œæ”¹è¿›å»ºè®®ï¼

## ğŸ“„ è®¸å¯è¯

æœ¬é¡¹ç›®é‡‡ç”¨ MIT è®¸å¯è¯ã€‚