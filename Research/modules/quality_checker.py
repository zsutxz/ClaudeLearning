"""
è´¨é‡æ£€æŸ¥æ¨¡å— - QualityChecker

æä¾›ç ”ç©¶æ•°æ®çš„è´¨é‡è¯„ä¼°å’ŒéªŒè¯åŠŸèƒ½ï¼Œæ”¯æŒ:
- å¤šç»´åº¦è´¨é‡è¯„ä¼°
- æ•°æ®å®Œæ•´æ€§æ£€æŸ¥
- å¯ä¿¡åº¦è¯„åˆ†
- æ”¹è¿›å»ºè®®ç”Ÿæˆ
"""

import os
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime, timedelta
import re
import json
from collections import defaultdict

# é…ç½®æ—¥å¿—
logger = logging.getLogger(__name__)

@dataclass
class QualityConfig:
    """è´¨é‡æ£€æŸ¥é…ç½®"""
    min_sources_required: int = 3
    min_confidence_threshold: float = 0.6
    enable_source_validation: bool = True
    enable_content_validation: bool = True
    enable_consistency_check: bool = True
    enable_bias_detection: bool = True

@dataclass
class QualityScore:
    """è´¨é‡åˆ†æ•°ç±»"""
    overall_score: float
    dimension_scores: Dict[str, float]
    issues: List[str]
    recommendations: List[str]
    confidence: float

class QualityChecker:
    """
    è´¨é‡æ£€æŸ¥å™¨

    è¯„ä¼°ç ”ç©¶æ•°æ®çš„å¯é æ€§ã€å®Œæ•´æ€§å’Œå‡†ç¡®æ€§
    """

    def __init__(self, research_agent):
        """
        åˆå§‹åŒ–è´¨é‡æ£€æŸ¥å™¨

        Args:
            research_agent: ResearchAgentå®ä¾‹
        """
        self.research_agent = research_agent
        self.config = QualityConfig()

        # è´¨é‡ç»´åº¦æƒé‡
        self.dimension_weights = {
            'completeness': 0.25,    # å®Œæ•´æ€§
            'reliability': 0.25,     # å¯é æ€§
            'relevance': 0.20,       # ç›¸å…³æ€§
            'freshness': 0.15,       # æ—¶æ•ˆæ€§
            'consistency': 0.15      # ä¸€è‡´æ€§
        }

        # è´¨é‡æ£€æŸ¥å†å²
        self.check_history = []

        logger.info("QualityChecker åˆå§‹åŒ–å®Œæˆ")

    async def check(self, research_data: Dict[str, Any]) -> QualityScore:
        """
        æ‰§è¡Œå…¨é¢çš„è´¨é‡æ£€æŸ¥

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            QualityScore: è´¨é‡è¯„ä¼°ç»“æœ
        """
        try:
            logger.info("å¼€å§‹æ‰§è¡Œè´¨é‡æ£€æŸ¥")

            # 1. åŸºç¡€æ•°æ®éªŒè¯
            basic_validation = self._basic_data_validation(research_data)

            # 2. å¤šç»´åº¦è´¨é‡è¯„ä¼°
            dimension_scores = self._assess_dimensions(research_data)

            # 3. é—®é¢˜è¯†åˆ«
            issues = self._identify_issues(research_data, dimension_scores)

            # 4. ç”Ÿæˆæ”¹è¿›å»ºè®®
            recommendations = self._generate_recommendations(issues, dimension_scores)

            # 5. è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°
            overall_score = self._calculate_overall_score(dimension_scores)

            # 6. è®¡ç®—ç½®ä¿¡åº¦
            confidence = self._calculate_confidence(research_data, overall_score)

            # æ„å»ºè´¨é‡åˆ†æ•°å¯¹è±¡
            quality_score = QualityScore(
                overall_score=overall_score,
                dimension_scores=dimension_scores,
                issues=issues,
                recommendations=recommendations,
                confidence=confidence
            )

            # è®°å½•æ£€æŸ¥å†å²
            self.check_history.append({
                'timestamp': datetime.now(),
                'overall_score': overall_score,
                'issues_count': len(issues),
                'confidence': confidence
            })

            logger.info(f"è´¨é‡æ£€æŸ¥å®Œæˆ - æ€»åˆ†: {overall_score:.2f}, ç½®ä¿¡åº¦: {confidence:.2f}")
            return quality_score

        except Exception as e:
            logger.error(f"è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}")
            return QualityScore(
                overall_score=0.0,
                dimension_scores={},
                issues=[f"è´¨é‡æ£€æŸ¥å¤±è´¥: {str(e)}"],
                recommendations=["è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§"],
                confidence=0.0
            )

    def _basic_data_validation(self, research_data: Dict[str, Any]) -> bool:
        """
        åŸºç¡€æ•°æ®éªŒè¯

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            bool: éªŒè¯æ˜¯å¦é€šè¿‡
        """
        required_fields = ['query', 'literature', 'data', 'analysis']

        for field in required_fields:
            if field not in research_data:
                logger.warning(f"ç¼ºå°‘å¿…éœ€å­—æ®µ: {field}")
                return False

        # æ£€æŸ¥æŸ¥è¯¢å†…å®¹
        query = research_data.get('query', '').strip()
        if len(query) < 3:
            logger.warning("æŸ¥è¯¢å†…å®¹è¿‡çŸ­")
            return False

        return True

    def _assess_dimensions(self, research_data: Dict[str, Any]) -> Dict[str, float]:
        """
        å¤šç»´åº¦è´¨é‡è¯„ä¼°

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            Dict[str, float]: å„ç»´åº¦çš„è´¨é‡åˆ†æ•°
        """
        scores = {}

        # 1. å®Œæ•´æ€§è¯„ä¼°
        scores['completeness'] = self._assess_completeness(research_data)

        # 2. å¯é æ€§è¯„ä¼°
        scores['reliability'] = self._assess_reliability(research_data)

        # 3. ç›¸å…³æ€§è¯„ä¼°
        scores['relevance'] = self._assess_relevance(research_data)

        # 4. æ—¶æ•ˆæ€§è¯„ä¼°
        scores['freshness'] = self._assess_freshness(research_data)

        # 5. ä¸€è‡´æ€§è¯„ä¼°
        scores['consistency'] = self._assess_consistency(research_data)

        return scores

    def _assess_completeness(self, research_data: Dict[str, Any]) -> float:
        """
        è¯„ä¼°æ•°æ®å®Œæ•´æ€§

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            float: å®Œæ•´æ€§åˆ†æ•° (0-10)
        """
        score = 5.0  # åŸºç¡€åˆ†æ•°

        # æ£€æŸ¥æ–‡çŒ®æ•°æ®å®Œæ•´æ€§
        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            # æ£€æŸ¥æ˜¯å¦æœ‰å¤šä¸ªæ•°æ®æº
            source_count = 0
            if literature.get('github_results'):
                source_count += 1
            if literature.get('paper_results'):
                source_count += 1
            if literature.get('blog_results'):
                source_count += 1

            if source_count >= 3:
                score += 2.0
            elif source_count >= 2:
                score += 1.0

            # æ£€æŸ¥æ•°æ®é‡
            total_results = (
                len(literature.get('github_results', [])) +
                len(literature.get('paper_results', [])) +
                len(literature.get('blog_results', []))
            )

            if total_results >= 20:
                score += 2.0
            elif total_results >= 10:
                score += 1.0
            elif total_results >= 5:
                score += 0.5

        # æ£€æŸ¥åˆ†ææ•°æ®å®Œæ•´æ€§
        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict):
            if analysis.get('analysis_report'):
                score += 0.5
            if analysis.get('key_findings'):
                score += 0.5

        # æ£€æŸ¥å…ƒæ•°æ®
        metadata = research_data.get('metadata', {})
        if isinstance(metadata, dict):
            required_metadata = ['research_domain', 'provider', 'model']
            metadata_completeness = sum(1 for field in required_metadata if field in metadata)
            score += metadata_completeness * 0.2

        return min(max(score, 0.0), 10.0)

    def _assess_reliability(self, research_data: Dict[str, Any]) -> float:
        """
        è¯„ä¼°æ•°æ®å¯é æ€§

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            float: å¯é æ€§åˆ†æ•° (0-10)
        """
        score = 5.0  # åŸºç¡€åˆ†æ•°

        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            # GitHubé¡¹ç›®å¯é æ€§
            github_results = literature.get('github_results', [])
            if github_results:
                # è¯„ä¼°é¡¹ç›®è´¨é‡
                high_quality_projects = 0
                for repo in github_results:
                    if hasattr(repo, 'metadata'):
                        meta = repo.metadata
                        stars = meta.get('stars', 0)
                        forks = meta.get('forks', 0)

                        # æ ¹æ®æ˜Ÿæ ‡å’Œåˆ†æ”¯æ•°è¯„ä¼°è´¨é‡
                        if stars > 1000:
                            high_quality_projects += 1
                        elif stars > 100:
                            high_quality_projects += 0.5

                if len(github_results) > 0:
                    github_quality_ratio = high_quality_projects / len(github_results)
                    score += github_quality_ratio * 2.0

            # å­¦æœ¯è®ºæ–‡å¯é æ€§
            paper_results = literature.get('paper_results', [])
            if paper_results:
                # å­¦æœ¯è®ºæ–‡é€šå¸¸å…·æœ‰è¾ƒé«˜çš„å¯é æ€§
                score += min(len(paper_results) * 0.5, 2.0)

        # AIæ¨¡å‹å¯é æ€§
        provider = research_data.get('metadata', {}).get('provider', '').lower()
        if provider in ['claude', 'openai', 'anthropic']:
            score += 1.0
        elif provider:
            score += 0.5

        return min(max(score, 0.0), 10.0)

    def _assess_relevance(self, research_data: Dict[str, Any]) -> float:
        """
        è¯„ä¼°æ•°æ®ç›¸å…³æ€§

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            float: ç›¸å…³æ€§åˆ†æ•° (0-10)
        """
        score = 5.0  # åŸºç¡€åˆ†æ•°

        query = research_data.get('query', '').lower()
        if not query:
            return 0.0

        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            relevance_scores = []

            # æ£€æŸ¥GitHubé¡¹ç›®ç›¸å…³æ€§
            for repo in literature.get('github_results', []):
                if hasattr(repo, 'title') and hasattr(repo, 'description'):
                    title_text = f"{repo.title} {repo.description}".lower()
                    relevance = self._calculate_text_relevance(query, title_text)
                    relevance_scores.append(relevance)

            # æ£€æŸ¥è®ºæ–‡ç›¸å…³æ€§
            for paper in literature.get('paper_results', []):
                if hasattr(paper, 'title') and hasattr(paper, 'description'):
                    title_text = f"{paper.title} {paper.description}".lower()
                    relevance = self._calculate_text_relevance(query, title_text)
                    relevance_scores.append(relevance)

            # æ£€æŸ¥åšå®¢ç›¸å…³æ€§
            for blog in literature.get('blog_results', []):
                if hasattr(blog, 'title') and hasattr(blog, 'description'):
                    title_text = f"{blog.title} {blog.description}".lower()
                    relevance = self._calculate_text_relevance(query, title_text)
                    relevance_scores.append(relevance)

            if relevance_scores:
                avg_relevance = sum(relevance_scores) / len(relevance_scores)
                score += avg_relevance * 3.0

        # åˆ†æå†…å®¹ç›¸å…³æ€§
        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict):
            analysis_text = analysis.get('analysis_report', '').lower()
            if analysis_text:
                analysis_relevance = self._calculate_text_relevance(query, analysis_text)
                score += analysis_relevance * 2.0

        return min(max(score, 0.0), 10.0)

    def _assess_freshness(self, research_data: Dict[str, Any]) -> float:
        """
        è¯„ä¼°æ•°æ®æ—¶æ•ˆæ€§

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            float: æ—¶æ•ˆæ€§åˆ†æ•° (0-10)
        """
        score = 5.0  # åŸºç¡€åˆ†æ•°

        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            timestamps = []

            # æ”¶é›†æ‰€æœ‰æ—¶é—´æˆ³
            for repo in literature.get('github_results', []):
                if hasattr(repo, 'timestamp'):
                    timestamps.append(repo.timestamp)

            for paper in literature.get('paper_results', []):
                if hasattr(paper, 'timestamp'):
                    timestamps.append(paper.timestamp)

            for blog in literature.get('blog_results', []):
                if hasattr(blog, 'timestamp'):
                    timestamps.append(blog.timestamp)

            if timestamps:
                # è®¡ç®—å¹³å‡æ—¶æ•ˆæ€§
                now = datetime.now()
                freshness_scores = []

                for timestamp in timestamps:
                    if isinstance(timestamp, datetime):
                        days_old = (now - timestamp).days
                        if days_old <= 7:
                            freshness_scores.append(1.0)
                        elif days_old <= 30:
                            freshness_scores.append(0.8)
                        elif days_old <= 90:
                            freshness_scores.append(0.6)
                        elif days_old <= 180:
                            freshness_scores.append(0.4)
                        else:
                            freshness_scores.append(0.2)

                if freshness_scores:
                    avg_freshness = sum(freshness_scores) / len(freshness_scores)
                    score += avg_freshness * 3.0

        return min(max(score, 0.0), 10.0)

    def _assess_consistency(self, research_data: Dict[str, Any]) -> float:
        """
        è¯„ä¼°æ•°æ®ä¸€è‡´æ€§

        Args:
            research_data: ç ”ç©¶æ•°æ®

        Returns:
            float: ä¸€è‡´æ€§åˆ†æ•° (0-10)
        """
        score = 7.0  # åŸºç¡€åˆ†æ•°ï¼ŒAIç”Ÿæˆçš„ä¸€è‡´æ€§é€šå¸¸è¾ƒå¥½

        # æ£€æŸ¥ä¸åŒæ¥æºçš„ä¸€è‡´æ€§
        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            sources = []

            if literature.get('github_results'):
                sources.append('GitHub')
            if literature.get('paper_results'):
                sources.append('Academic')
            if literature.get('blog_results'):
                sources.append('Blogs')

            # å¤šä¸ªæ¥æºé€šå¸¸è¡¨ç¤ºæ›´å¥½çš„ä¸€è‡´æ€§éªŒè¯
            if len(sources) >= 3:
                score += 1.0
            elif len(sources) >= 2:
                score += 0.5

        # æ£€æŸ¥åˆ†æä¸€è‡´æ€§
        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict):
            # æ£€æŸ¥æ˜¯å¦æœ‰æ˜æ˜¾çŸ›ç›¾çš„åˆ†æç»“æœ
            if analysis.get('analysis_report'):
                score += 0.5

        return min(max(score, 0.0), 10.0)

    def _calculate_text_relevance(self, query: str, text: str) -> float:
        """
        è®¡ç®—æ–‡æœ¬ç›¸å…³æ€§

        Args:
            query: æŸ¥è¯¢æ–‡æœ¬
            text: ç›®æ ‡æ–‡æœ¬

        Returns:
            float: ç›¸å…³æ€§åˆ†æ•° (0-1)
        """
        query_words = set(query.split())
        text_words = set(text.split())

        if not query_words:
            return 0.0

        # è®¡ç®—è¯æ±‡é‡å åº¦
        intersection = query_words.intersection(text_words)
        overlap_ratio = len(intersection) / len(query_words)

        # è€ƒè™‘æ–‡æœ¬é•¿åº¦å½±å“
        length_factor = min(len(text) / 1000, 1.0)  # é¿å…è¿‡çŸ­æ–‡æœ¬çš„å½±å“

        return min(overlap_ratio * (1 + length_factor), 1.0)

    def _identify_issues(self, research_data: Dict[str, Any], dimension_scores: Dict[str, float]) -> List[str]:
        """
        è¯†åˆ«è´¨é‡é—®é¢˜

        Args:
            research_data: ç ”ç©¶æ•°æ®
            dimension_scores: å„ç»´åº¦åˆ†æ•°

        Returns:
            List[str]: é—®é¢˜åˆ—è¡¨
        """
        issues = []

        # åŸºäºç»´åº¦åˆ†æ•°è¯†åˆ«é—®é¢˜
        for dimension, score in dimension_scores.items():
            if score < 5.0:
                if dimension == 'completeness':
                    issues.append("æ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå°‘å…³é”®ä¿¡æ¯æˆ–æ•°æ®æº")
                elif dimension == 'reliability':
                    issues.append("æ•°æ®æ¥æºå¯é æ€§è¾ƒä½ï¼Œå»ºè®®å¢åŠ æƒå¨æ¥æº")
                elif dimension == 'relevance':
                    issues.append("æ•°æ®ä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³æ€§ä¸å¤Ÿå¼º")
                elif dimension == 'freshness':
                    issues.append("æ•°æ®æ—¶æ•ˆæ€§è¾ƒå·®ï¼Œéƒ¨åˆ†ä¿¡æ¯å¯èƒ½è¿‡æ—¶")
                elif dimension == 'consistency':
                    issues.append("ä¸åŒæ¥æºæ•°æ®å­˜åœ¨ä¸ä¸€è‡´æ€§")

        # æ£€æŸ¥å…·ä½“é—®é¢˜
        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            total_sources = (
                len(literature.get('github_results', [])) +
                len(literature.get('paper_results', [])) +
                len(literature.get('blog_results', []))
            )

            if total_sources < self.config.min_sources_required:
                issues.append(f"æ•°æ®æºæ•°é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦{self.config.min_sources_required}ä¸ªæ¥æº")

        # æ£€æŸ¥åˆ†ææ·±åº¦
        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict):
            if not analysis.get('analysis_report'):
                issues.append("ç¼ºå°‘æ·±å…¥çš„åˆ†ææŠ¥å‘Š")
            if not analysis.get('key_findings'):
                issues.append("ç¼ºå°‘å…³é”®å‘ç°æ€»ç»“")

        return issues

    def _generate_recommendations(self, issues: List[str], dimension_scores: Dict[str, float]) -> List[str]:
        """
        ç”Ÿæˆæ”¹è¿›å»ºè®®

        Args:
            issues: é—®é¢˜åˆ—è¡¨
            dimension_scores: å„ç»´åº¦åˆ†æ•°

        Returns:
            List[str]: å»ºè®®åˆ—è¡¨
        """
        recommendations = []

        # åŸºäºé—®é¢˜ç”Ÿæˆå»ºè®®
        if "æ•°æ®ä¸å®Œæ•´" in str(issues):
            recommendations.extend([
                "æ‰©å±•æ•°æ®æ”¶é›†èŒƒå›´ï¼Œå¢åŠ æ›´å¤šæƒå¨æ¥æº",
                "è¡¥å……å…³é”®æŒ‡æ ‡å’Œé‡åŒ–æ•°æ®",
                "å®Œå–„å…ƒæ•°æ®ä¿¡æ¯"
            ])

        if "å¯é æ€§è¾ƒä½" in str(issues):
            recommendations.extend([
                "ä¼˜å…ˆé€‰æ‹©åŒè¡Œè¯„è®®çš„å­¦æœ¯è®ºæ–‡",
                "å¢åŠ çŸ¥åä¼ä¸šå’Œæœºæ„çš„æŠ¥å‘Š",
                "éªŒè¯å¼€æºé¡¹ç›®çš„å½±å“åŠ›å’Œæ´»è·ƒåº¦"
            ])

        if "ç›¸å…³æ€§ä¸å¤Ÿå¼º" in str(issues):
            recommendations.extend([
                "ä¼˜åŒ–æœç´¢å…³é”®è¯å’ŒæŸ¥è¯¢ç­–ç•¥",
                "ä½¿ç”¨æ›´ä¸“ä¸šçš„æ£€ç´¢æ•°æ®åº“",
                "å¢åŠ é¢†åŸŸä¸“å®¶çš„æ¨èå’Œç­›é€‰"
            ])

        if "æ—¶æ•ˆæ€§è¾ƒå·®" in str(issues):
            recommendations.extend([
                "ä¼˜å…ˆæ”¶é›†æœ€è¿‘6ä¸ªæœˆçš„æ•°æ®",
                "è®¾ç½®æ—¶é—´è¿‡æ»¤å™¨ï¼Œæ’é™¤è¿‡æ—¶ä¿¡æ¯",
                "å…³æ³¨æœ€æ–°å‘å¸ƒçš„ç ”ç©¶æŠ¥å‘Šå’ŒæŠ€æœ¯è¶‹åŠ¿"
            ])

        # åŸºäºç»´åº¦åˆ†æ•°ç”Ÿæˆé€šç”¨å»ºè®®
        low_scores = [dim for dim, score in dimension_scores.items() if score < 6.0]
        if low_scores:
            recommendations.append(f"é‡ç‚¹æ”¹è¿›ä»¥ä¸‹æ–¹é¢: {', '.join(low_scores)}")

        # é«˜åˆ†å»ºè®®ï¼ˆä¿æŒä¼˜åŠ¿ï¼‰
        high_scores = [dim for dim, score in dimension_scores.items() if score >= 8.0]
        if high_scores:
            recommendations.append(f"ç»§ç»­ä¿æŒä»¥ä¸‹æ–¹é¢çš„ä¼˜åŠ¿: {', '.join(high_scores)}")

        return list(set(recommendations))  # å»é‡

    def _calculate_overall_score(self, dimension_scores: Dict[str, float]) -> float:
        """
        è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°

        Args:
            dimension_scores: å„ç»´åº¦åˆ†æ•°

        Returns:
            float: æ€»ä½“åˆ†æ•° (0-10)
        """
        if not dimension_scores:
            return 0.0

        weighted_sum = sum(
            score * self.dimension_weights.get(dimension, 0.2)
            for dimension, score in dimension_scores.items()
        )

        return min(max(weighted_sum, 0.0), 10.0)

    def _calculate_confidence(self, research_data: Dict[str, Any], overall_score: float) -> float:
        """
        è®¡ç®—è¯„ä¼°ç½®ä¿¡åº¦

        Args:
            research_data: ç ”ç©¶æ•°æ®
            overall_score: æ€»ä½“åˆ†æ•°

        Returns:
            float: ç½®ä¿¡åº¦ (0-1)
        """
        confidence = 0.5  # åŸºç¡€ç½®ä¿¡åº¦

        # åŸºäºæ•°æ®é‡è°ƒæ•´ç½®ä¿¡åº¦
        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            total_sources = (
                len(literature.get('github_results', [])) +
                len(literature.get('paper_results', [])) +
                len(literature.get('blog_results', []))
            )

            if total_sources >= 20:
                confidence += 0.3
            elif total_sources >= 10:
                confidence += 0.2
            elif total_sources >= 5:
                confidence += 0.1

        # åŸºäºåˆ†æ•°ä¸€è‡´æ€§è°ƒæ•´ç½®ä¿¡åº¦
        dimension_scores = self._assess_dimensions(research_data)
        if dimension_scores:
            score_variance = max(dimension_scores.values()) - min(dimension_scores.values())
            if score_variance < 2.0:
                confidence += 0.2
            elif score_variance > 5.0:
                confidence -= 0.2

        # åŸºäºæ€»ä½“åˆ†æ•°è°ƒæ•´
        if overall_score >= 8.0:
            confidence += 0.1
        elif overall_score < 4.0:
            confidence -= 0.2

        return min(max(confidence, 0.0), 1.0)

    def get_quality_summary(self, quality_score: QualityScore) -> str:
        """
        è·å–è´¨é‡è¯„ä¼°æ‘˜è¦

        Args:
            quality_score: è´¨é‡åˆ†æ•°å¯¹è±¡

        Returns:
            str: è´¨é‡æ‘˜è¦
        """
        summary = f"""## è´¨é‡è¯„ä¼°æ‘˜è¦

**æ€»ä½“åˆ†æ•°**: {quality_score.overall_score:.2f}/10.0
**è¯„ä¼°ç½®ä¿¡åº¦**: {quality_score.confidence:.2f}/1.0

### å„ç»´åº¦è¯„åˆ†

"""

        for dimension, score in quality_score.dimension_scores.items():
            dimension_names = {
                'completeness': 'å®Œæ•´æ€§',
                'reliability': 'å¯é æ€§',
                'relevance': 'ç›¸å…³æ€§',
                'freshness': 'æ—¶æ•ˆæ€§',
                'consistency': 'ä¸€è‡´æ€§'
            }
            display_name = dimension_names.get(dimension, dimension)
            summary += f"- **{display_name}**: {score:.2f}/10.0\n"

        if quality_score.issues:
            summary += "\n### å‘ç°çš„é—®é¢˜\n\n"
            for issue in quality_score.issues:
                summary += f"- âš ï¸ {issue}\n"

        if quality_score.recommendations:
            summary += "\n### æ”¹è¿›å»ºè®®\n\n"
            for recommendation in quality_score.recommendations:
                summary += f"- ğŸ’¡ {recommendation}\n"

        return summary

    async def get_check_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """
        è·å–è´¨é‡æ£€æŸ¥å†å²

        Args:
            limit: è¿”å›çš„è®°å½•æ•°é‡

        Returns:
            List[Dict]: æ£€æŸ¥å†å²è®°å½•
        """
        return self.check_history[-limit:] if self.check_history else []

    def clear_check_history(self):
        """æ¸…ç©ºè´¨é‡æ£€æŸ¥å†å²"""
        self.check_history.clear()
        logger.info("è´¨é‡æ£€æŸ¥å†å²å·²æ¸…ç©º")