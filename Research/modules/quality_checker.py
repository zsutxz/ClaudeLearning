"""
è´¨é‡æ£€æŸ¥æ¨¡å— - QualityChecker
æä¾›ç ”ç©¶æ•°æ®çš„è´¨é‡è¯„ä¼°å’ŒéªŒè¯åŠŸèƒ½ã€‚
"""

import os
import asyncio
import logging
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass
from datetime import datetime
from collections import defaultdict

logger = logging.getLogger(__name__)

@dataclass
class QualityConfig:
    """è´¨é‡æ£€æŸ¥é…ç½®"""
    min_sources_required: int = 3
    min_confidence_threshold: float = 0.6
    enable_source_validation: bool = True
    enable_content_validation: bool = True
    enable_consistency_check: bool = True
    enable_bias_detection: bool = True

@dataclass
class QualityScore:
    """è´¨é‡åˆ†æ•°ç±»"""
    overall_score: float
    dimension_scores: Dict[str, float]
    issues: List[str]
    recommendations: List[str]
    confidence: float

class QualityChecker:
    """è´¨é‡æ£€æŸ¥å™¨ - è¯„ä¼°ç ”ç©¶æ•°æ®çš„å¯é æ€§å’Œå®Œæ•´æ€§"""

    def __init__(self, research_agent):
        self.research_agent = research_agent
        self.config = QualityConfig()

        # è´¨é‡ç»´åº¦æƒé‡
        self.dimension_weights = {
            'completeness': 0.25,
            'reliability': 0.25,
            'relevance': 0.20,
            'freshness': 0.15,
            'consistency': 0.15
        }

        self.check_history = []
        logger.info("QualityChecker åˆå§‹åŒ–å®Œæˆ")

    async def check(self, research_data: Dict[str, Any]) -> QualityScore:
        """æ‰§è¡Œå…¨é¢çš„è´¨é‡æ£€æŸ¥"""
        try:
            logger.info("å¼€å§‹æ‰§è¡Œè´¨é‡æ£€æŸ¥")

            # æ‰§è¡Œå„é¡¹æ£€æŸ¥
            dimension_scores = self._assess_dimensions(research_data)
            issues = self._identify_issues(research_data, dimension_scores)
            recommendations = self._generate_recommendations(issues, dimension_scores)
            overall_score = self._calculate_overall_score(dimension_scores)
            confidence = self._calculate_confidence(research_data, overall_score)

            quality_score = QualityScore(
                overall_score=overall_score,
                dimension_scores=dimension_scores,
                issues=issues,
                recommendations=recommendations,
                confidence=confidence
            )

            self.check_history.append({
                'timestamp': datetime.now(),
                'overall_score': overall_score,
                'issues_count': len(issues),
                'confidence': confidence
            })

            logger.info(f"è´¨é‡æ£€æŸ¥å®Œæˆ - æ€»åˆ†: {overall_score:.2f}, ç½®ä¿¡åº¦: {confidence:.2f}")
            return quality_score

        except Exception as e:
            logger.error(f"è´¨é‡æ£€æŸ¥å¤±è´¥: {e}")
            return QualityScore(
                overall_score=0.0, dimension_scores={}, issues=[f"è´¨é‡æ£€æŸ¥å¤±è´¥: {e}"],
                recommendations=["è¯·æ£€æŸ¥æ•°æ®æ ¼å¼å’Œå®Œæ•´æ€§"], confidence=0.0
            )

    def _assess_dimensions(self, research_data: Dict[str, Any]) -> Dict[str, float]:
        """å¤šç»´åº¦è´¨é‡è¯„ä¼°"""
        return {
            'completeness': self._assess_completeness(research_data),
            'reliability': self._assess_reliability(research_data),
            'relevance': self._assess_relevance(research_data),
            'freshness': self._assess_freshness(research_data),
            'consistency': self._assess_consistency(research_data)
        }

    def _assess_completeness(self, research_data: Dict[str, Any]) -> float:
        """è¯„ä¼°æ•°æ®å®Œæ•´æ€§"""
        score = 5.0
        literature = research_data.get('literature', {})

        if isinstance(literature, dict):
            source_count = sum(1 for k in ['github_results', 'paper_results', 'blog_results'] if literature.get(k))
            if source_count >= 3:
                score += 2.0
            elif source_count >= 2:
                score += 1.0

            total_results = sum(len(literature.get(k, [])) for k in ['github_results', 'paper_results', 'blog_results'])
            if total_results >= 20:
                score += 2.0
            elif total_results >= 10:
                score += 1.0
            elif total_results >= 5:
                score += 0.5

        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict):
            if analysis.get('analysis_report'):
                score += 0.5
            if analysis.get('key_findings'):
                score += 0.5

        return min(max(score, 0.0), 10.0)

    def _assess_reliability(self, research_data: Dict[str, Any]) -> float:
        """è¯„ä¼°æ•°æ®å¯é æ€§"""
        score = 5.0
        literature = research_data.get('literature', {})

        if isinstance(literature, dict):
            github_results = literature.get('github_results', [])
            if github_results:
                high_quality = sum(1 for r in github_results if hasattr(r, 'metadata') and r.metadata.get('stars', 0) > 100)
                if github_results:
                    score += (high_quality / len(github_results)) * 2.0

            paper_results = literature.get('paper_results', [])
            if paper_results:
                score += min(len(paper_results) * 0.5, 2.0)

        provider = research_data.get('metadata', {}).get('provider', '').lower()
        if provider in ['claude', 'openai', 'anthropic']:
            score += 1.0
        elif provider:
            score += 0.5

        return min(max(score, 0.0), 10.0)

    def _assess_relevance(self, research_data: Dict[str, Any]) -> float:
        """è¯„ä¼°æ•°æ®ç›¸å…³æ€§"""
        score = 5.0
        query = research_data.get('query', '').lower()
        if not query:
            return 0.0

        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            relevance_scores = []

            for result_type in ['github_results', 'paper_results', 'blog_results']:
                for item in literature.get(result_type, []):
                    if hasattr(item, 'title') and hasattr(item, 'description'):
                        text = f"{item.title} {item.description}".lower()
                        relevance = self._calculate_text_relevance(query, text)
                        relevance_scores.append(relevance)

            if relevance_scores:
                score += (sum(relevance_scores) / len(relevance_scores)) * 3.0

        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict):
            analysis_text = analysis.get('analysis_report', '').lower()
            if analysis_text:
                score += self._calculate_text_relevance(query, analysis_text) * 2.0

        return min(max(score, 0.0), 10.0)

    def _assess_freshness(self, research_data: Dict[str, Any]) -> float:
        """è¯„ä¼°æ•°æ®æ—¶æ•ˆæ€§"""
        score = 5.0
        literature = research_data.get('literature', {})

        if isinstance(literature, dict):
            timestamps = []
            for result_type in ['github_results', 'paper_results', 'blog_results']:
                for item in literature.get(result_type, []):
                    if hasattr(item, 'timestamp') and isinstance(item.timestamp, datetime):
                        timestamps.append(item.timestamp)

            if timestamps:
                now = datetime.now()
                freshness_scores = []
                for ts in timestamps:
                    days_old = (now - ts).days
                    if days_old <= 7:
                        freshness_scores.append(1.0)
                    elif days_old <= 30:
                        freshness_scores.append(0.8)
                    elif days_old <= 90:
                        freshness_scores.append(0.6)
                    elif days_old <= 180:
                        freshness_scores.append(0.4)
                    else:
                        freshness_scores.append(0.2)

                if freshness_scores:
                    score += (sum(freshness_scores) / len(freshness_scores)) * 3.0

        return min(max(score, 0.0), 10.0)

    def _assess_consistency(self, research_data: Dict[str, Any]) -> float:
        """è¯„ä¼°æ•°æ®ä¸€è‡´æ€§"""
        score = 7.0

        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            sources = [k for k in ['github_results', 'paper_results', 'blog_results'] if literature.get(k)]
            if len(sources) >= 3:
                score += 1.0
            elif len(sources) >= 2:
                score += 0.5

        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict) and analysis.get('analysis_report'):
            score += 0.5

        return min(max(score, 0.0), 10.0)

    def _calculate_text_relevance(self, query: str, text: str) -> float:
        """è®¡ç®—æ–‡æœ¬ç›¸å…³æ€§"""
        query_words = set(query.split())
        text_words = set(text.split())

        if not query_words:
            return 0.0

        intersection = query_words.intersection(text_words)
        overlap_ratio = len(intersection) / len(query_words)
        length_factor = min(len(text) / 1000, 1.0)

        return min(overlap_ratio * (1 + length_factor), 1.0)

    def _identify_issues(self, research_data: Dict[str, Any], dimension_scores: Dict[str, float]) -> List[str]:
        """è¯†åˆ«è´¨é‡é—®é¢˜"""
        issues = []

        dimension_names = {
            'completeness': 'æ•°æ®ä¸å®Œæ•´ï¼Œç¼ºå°‘å…³é”®ä¿¡æ¯æˆ–æ•°æ®æº',
            'reliability': 'æ•°æ®æ¥æºå¯é æ€§è¾ƒä½ï¼Œå»ºè®®å¢åŠ æƒå¨æ¥æº',
            'relevance': 'æ•°æ®ä¸ç ”ç©¶ä¸»é¢˜ç›¸å…³æ€§ä¸å¤Ÿå¼º',
            'freshness': 'æ•°æ®æ—¶æ•ˆæ€§è¾ƒå·®ï¼Œéƒ¨åˆ†ä¿¡æ¯å¯èƒ½è¿‡æ—¶',
            'consistency': 'ä¸åŒæ¥æºæ•°æ®å­˜åœ¨ä¸ä¸€è‡´æ€§'
        }

        for dimension, score in dimension_scores.items():
            if score < 5.0:
                issues.append(dimension_names.get(dimension, f"{dimension}åˆ†æ•°è¾ƒä½"))

        # æ£€æŸ¥æ•°æ®æºæ•°é‡
        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            total_sources = sum(len(literature.get(k, [])) for k in ['github_results', 'paper_results', 'blog_results'])
            if total_sources < self.config.min_sources_required:
                issues.append(f"æ•°æ®æºæ•°é‡ä¸è¶³ï¼Œè‡³å°‘éœ€è¦{self.config.min_sources_required}ä¸ªæ¥æº")

        # æ£€æŸ¥åˆ†ææ·±åº¦
        analysis = research_data.get('analysis', {})
        if isinstance(analysis, dict):
            if not analysis.get('analysis_report'):
                issues.append("ç¼ºå°‘æ·±å…¥çš„åˆ†ææŠ¥å‘Š")
            if not analysis.get('key_findings'):
                issues.append("ç¼ºå°‘å…³é”®å‘ç°æ€»ç»“")

        return issues

    def _generate_recommendations(self, issues: List[str], dimension_scores: Dict[str, float]) -> List[str]:
        """ç”Ÿæˆæ”¹è¿›å»ºè®®"""
        recommendations = []

        issue_recommendations = {
            "æ•°æ®ä¸å®Œæ•´": ["æ‰©å±•æ•°æ®æ”¶é›†èŒƒå›´", "è¡¥å……å…³é”®æŒ‡æ ‡", "å®Œå–„å…ƒæ•°æ®"],
            "å¯é æ€§è¾ƒä½": ["é€‰æ‹©åŒè¡Œè¯„è®®è®ºæ–‡", "å¢åŠ ä¼ä¸šæŠ¥å‘Š", "éªŒè¯é¡¹ç›®å½±å“åŠ›"],
            "ç›¸å…³æ€§ä¸å¤Ÿå¼º": ["ä¼˜åŒ–æœç´¢å…³é”®è¯", "ä½¿ç”¨ä¸“ä¸šæ•°æ®åº“", "å¢åŠ ä¸“å®¶æ¨è"],
            "æ—¶æ•ˆæ€§è¾ƒå·®": ["æ”¶é›†æœ€è¿‘6ä¸ªæœˆæ•°æ®", "è®¾ç½®æ—¶é—´è¿‡æ»¤å™¨", "å…³æ³¨æœ€æ–°åŠ¨æ€"]
        }

        for issue in issues:
            for keyword, recs in issue_recommendations.items():
                if keyword in issue:
                    recommendations.extend(recs)

        # åŸºäºç»´åº¦åˆ†æ•°ç”Ÿæˆå»ºè®®
        low_scores = [dim for dim, score in dimension_scores.items() if score < 6.0]
        if low_scores:
            recommendations.append(f"é‡ç‚¹æ”¹è¿›: {', '.join(low_scores)}")

        high_scores = [dim for dim, score in dimension_scores.items() if score >= 8.0]
        if high_scores:
            recommendations.append(f"ä¿æŒä¼˜åŠ¿: {', '.join(high_scores)}")

        return list(set(recommendations))

    def _calculate_overall_score(self, dimension_scores: Dict[str, float]) -> float:
        """è®¡ç®—æ€»ä½“è´¨é‡åˆ†æ•°"""
        if not dimension_scores:
            return 0.0

        weighted_sum = sum(
            score * self.dimension_weights.get(dimension, 0.2)
            for dimension, score in dimension_scores.items()
        )

        return min(max(weighted_sum, 0.0), 10.0)

    def _calculate_confidence(self, research_data: Dict[str, Any], overall_score: float) -> float:
        """è®¡ç®—è¯„ä¼°ç½®ä¿¡åº¦"""
        confidence = 0.5

        literature = research_data.get('literature', {})
        if isinstance(literature, dict):
            total_sources = sum(len(literature.get(k, [])) for k in ['github_results', 'paper_results', 'blog_results'])

            if total_sources >= 20:
                confidence += 0.3
            elif total_sources >= 10:
                confidence += 0.2
            elif total_sources >= 5:
                confidence += 0.1

        if overall_score >= 8.0:
            confidence += 0.1
        elif overall_score < 4.0:
            confidence -= 0.2

        return min(max(confidence, 0.0), 1.0)

    def get_quality_summary(self, quality_score: QualityScore) -> str:
        """è·å–è´¨é‡è¯„ä¼°æ‘˜è¦"""
        dimension_names = {
            'completeness': 'å®Œæ•´æ€§', 'reliability': 'å¯é æ€§', 'relevance': 'ç›¸å…³æ€§',
            'freshness': 'æ—¶æ•ˆæ€§', 'consistency': 'ä¸€è‡´æ€§'
        }

        summary = f"## è´¨é‡è¯„ä¼°æ‘˜è¦\n\n**æ€»ä½“åˆ†æ•°**: {quality_score.overall_score:.2f}/10.0\n**è¯„ä¼°ç½®ä¿¡åº¦**: {quality_score.confidence:.2f}/1.0\n\n### å„ç»´åº¦è¯„åˆ†\n\n"

        for dimension, score in quality_score.dimension_scores.items():
            display_name = dimension_names.get(dimension, dimension)
            summary += f"- **{display_name}**: {score:.2f}/10.0\n"

        if quality_score.issues:
            summary += "\n### å‘ç°çš„é—®é¢˜\n\n"
            for issue in quality_score.issues:
                summary += f"- âš ï¸ {issue}\n"

        if quality_score.recommendations:
            summary += "\n### æ”¹è¿›å»ºè®®\n\n"
            for recommendation in quality_score.recommendations:
                summary += f"- ğŸ’¡ {recommendation}\n"

        return summary

    async def get_check_history(self, limit: int = 10) -> List[Dict[str, Any]]:
        """è·å–è´¨é‡æ£€æŸ¥å†å²"""
        return self.check_history[-limit:] if self.check_history else []

    def clear_check_history(self):
        """æ¸…ç©ºè´¨é‡æ£€æŸ¥å†å²"""
        self.check_history.clear()
        logger.info("è´¨é‡æ£€æŸ¥å†å²å·²æ¸…ç©º")
